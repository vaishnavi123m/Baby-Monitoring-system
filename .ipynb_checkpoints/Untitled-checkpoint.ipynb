{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b2e38b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape before reshaping: (800, 150528)\n",
      "X_train shape after reshaping: (800, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset generation function\n",
    "def generate_synthetic_data(num_samples=4000):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generating synthetic features (SpO2 and HeartRate)\n",
    "    SpO2 = np.random.uniform(90, 100, num_samples)  # Simulate SpO2 values between 90 and 100\n",
    "    HeartRate = np.random.uniform(50, 120, num_samples)  # Simulate HeartRate between 50 and 120\n",
    "    \n",
    "    # Simulating Labels (0: Normal, 1: Mild Hypoxia, 2: Severe Hypoxia)\n",
    "    labels = np.random.choice([0, 1, 2], num_samples)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'SpO2': SpO2,\n",
    "        'HeartRate': HeartRate,\n",
    "        'Label': labels\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Preprocess the data (normalizing and reshaping)\n",
    "def preprocess_data(data):\n",
    "    X = data[[\"SpO2\", \"HeartRate\"]].values\n",
    "    y = to_categorical(data[\"Label\"].values, num_classes=3)  # One-hot encode the labels\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Padding to make the number of elements fit ResNet input size\n",
    "    # We want to get a shape of (224, 224, 3)\n",
    "    \n",
    "    # Ensure X_train and X_test have 150528 features (224 * 224 * 3)\n",
    "    target_size = 224 * 224 * 3\n",
    "    total_elements_train = X_train.shape[1]\n",
    "    total_elements_test = X_test.shape[1]\n",
    "    \n",
    "    # Check if the current size is already multiple of target_size, else pad\n",
    "    padding_needed_train = target_size - total_elements_train % target_size\n",
    "    padding_needed_test = target_size - total_elements_test % target_size\n",
    "    \n",
    "    X_train = np.pad(X_train, ((0, 0), (0, padding_needed_train)), mode='constant')\n",
    "    X_test = np.pad(X_test, ((0, 0), (0, padding_needed_test)), mode='constant')\n",
    "    \n",
    "    print(f\"X_train shape before reshaping: {X_train.shape}\")\n",
    "    \n",
    "    # Reshape data into (224, 224, 3)\n",
    "    X_train = np.reshape(X_train, (-1, 224, 224, 3))  \n",
    "    X_test = np.reshape(X_test, (-1, 224, 224, 3))\n",
    "    \n",
    "    print(f\"X_train shape after reshaping: {X_train.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "data = generate_synthetic_data(num_samples=1000)  # Generate synthetic data with 1000 samples\n",
    "X_train, X_test, y_train, y_test = preprocess_data(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0661caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the model\n",
    "def build_model(input_shape=(224, 224, 3)):\n",
    "    # Load pre-trained ResNet152 model, excluding the top layer\n",
    "    base_model = ResNet152(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top of ResNet152\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)  # Dropout to avoid overfitting\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(3, activation='softmax')(x)  # 3 classes for classification\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage: build the model\n",
    "model = build_model()\n",
    "model.summary()  # View model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf9101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d766524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"Test Loss: {loss}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Example usage: Evaluate the model\n",
    "evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot history\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ce8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
